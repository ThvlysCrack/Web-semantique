import spacy

# Charger le mod√®le NLP de spaCy
nlp = spacy.load("en_core_web_trf")

def correct_pos(token, context):
    """
    Corrige les erreurs courantes de POS-tags en fonction du contexte global.
    """
    if token.text in ["Where", "When"]:
        return "ADV"  # "Where" et "When" sont toujours des adverbes interrogatifs
    if token.text in ["Who", "What"]:
        return "PRON"  # "Who" et "What" sont des pronoms
    if token.text in ["Which"] and context[token.i + 1].pos_ == "NOUN":
        return "DET"  # "Which" est un d√©terminant avant un nom
    return token.pos_  # Sinon, garder le POS d'origine

def preprocess_question(question):
    """
    Pr√©traitement avanc√© des questions :
    - Tokenisation et lemmatisation
    - Fusion des entit√©s multi-mots automatiquement
    - √âtiquetage grammatical (POS-tagging)
    - D√©tection des entit√©s nomm√©es
    - Correction automatique des POS-tags et des entit√©s mal class√©es
    """
    doc = nlp(question)

    # üìå Fusion des entit√©s multi-mots d√©tect√©es par spaCy
    entity_dict = {ent.text: ent.label_ for ent in doc.ents}

    # üìå Fusion des noms propres adjacents (ex: "Star Wars", "New York")
    merged_tokens = []
    merged_lemmas = []
    merged_pos_tags = []
    merged_entities = []
    
    i = 0
    while i < len(doc):
        token = doc[i]
        found_entity = False

        # üìå V√©rifier si ce token appartient √† une entit√© multi-mots
        for ent_text in entity_dict:
            ent_words = ent_text.split()
            if doc[i:i+len(ent_words)].text == ent_text:
                merged_tokens.append(ent_text)
                merged_lemmas.append(ent_text)  # Pas de lemmatisation pour les entit√©s
                merged_pos_tags.append((ent_text, "PROPN"))  # Une entit√© est toujours PROPN
                merged_entities.append((ent_text, entity_dict[ent_text]))  # Ajouter en tant qu'entit√©
                i += len(ent_words)  # Sauter les mots d√©j√† fusionn√©s
                found_entity = True
                break

        # üìå Fusion automatique des noms propres adjacents (ex: "Star Wars")
        if not found_entity and token.pos_ == "PROPN" and i + 1 < len(doc) and doc[i + 1].pos_ == "PROPN":
            entity_text = token.text + " " + doc[i + 1].text
            merged_tokens.append(entity_text)
            merged_lemmas.append(entity_text)
            merged_pos_tags.append((entity_text, "PROPN"))
            merged_entities.append((entity_text, "WORK_OF_ART"))  # Suppose qu'un nom propre fusionn√© est une entit√©
            i += 2
            continue

        if not found_entity:
            merged_tokens.append(token.text)
            merged_lemmas.append(token.lemma_)
            
            # üìå Appliquer la correction des POS-tags contextuellement
            corrected_pos = correct_pos(token, doc)
            merged_pos_tags.append((token.text, corrected_pos))
            
            i += 1

    return {
        "tokens": merged_tokens,
        "lemmas": merged_lemmas,
        "pos_tags": merged_pos_tags,
        "entities": merged_entities
    }

# üìå Liste de 10 questions g√©n√©rales pour tester
questions = [
    "Where is Fort Knox located?",
    "Who composed the music for Star Wars?",
    "What is the population of Germany?",
    "Which river flows through Paris?",
    "Who discovered penicillin?",
    "When was the Eiffel Tower built?",
    "What is the tallest mountain in the world?",
    "Which city is the capital of Japan?",
    "Who wrote the book The Catcher in the Rye?",
    "What is the currency used in Canada?"
]

# üìå Ex√©cuter le pr√©traitement sur chaque question
for i, question in enumerate(questions):
    print(f"\nüìù Question {i+1}: {question}")
    result = preprocess_question(question)
    
    for key, value in result.items():
        print(f"{key}: {value}")
