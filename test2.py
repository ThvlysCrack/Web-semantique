import spacy

# Charger le mod√®le NLP de spaCy
nlp = spacy.load("en_core_web_sm")

# üìå Correction manuelle des POS-tags pour les entit√©s et les mots-cl√©s courants
POS_CORRECTIONS = {
    "Where": "ADV",  # Correction pour "Where"
    "Who": "PRON",   # Correction pour "Who"
    "What": "PRON",  # Correction pour "What"
    "Which": "DET",  # Correction pour "Which"
}

def preprocess_question(question):
    """
    Fonction de pr√©traitement avanc√© :
    - Tokenisation avanc√©e
    - Fusion automatique des entit√©s multi-mots
    - Lemmatisation
    - √âtiquetage grammatical (POS-tagging)
    - D√©tection des entit√©s nomm√©es
    - Correction des POS-tags pour assurer la coh√©rence
    """
    doc = nlp(question)

    # üìå √âtape 1 : Extraire les entit√©s d√©tect√©es par spaCy
    entity_dict = {ent.text: ent.label_ for ent in doc.ents}

    # üìå √âtape 2 : Fusionner les entit√©s multi-mots d√©tect√©es
    merged_tokens = []
    merged_lemmas = []
    merged_pos_tags = []
    merged_entities = []
    
    i = 0
    while i < len(doc):
        token = doc[i]
        found_entity = False

        # V√©rifier si ce token appartient √† une entit√© multi-mots
        for ent_text in entity_dict:
            ent_words = ent_text.split()
            if doc[i:i+len(ent_words)].text == ent_text:
                merged_tokens.append(ent_text)
                merged_lemmas.append(ent_text)  # Pas de lemmatisation pour les entit√©s
                
                # üìå Correction manuelle des POS-tags si n√©cessaire
                corrected_pos = POS_CORRECTIONS.get(ent_text, "PROPN")
                merged_pos_tags.append((ent_text, corrected_pos))
                merged_entities.append((ent_text, entity_dict[ent_text]))  # Ajouter en tant qu'entit√©
                i += len(ent_words)  # Sauter les mots d√©j√† fusionn√©s
                found_entity = True
                break

        if not found_entity:
            merged_tokens.append(token.text)
            merged_lemmas.append(token.lemma_)
            
            # üìå Appliquer une correction si le token a une mauvaise classification POS
            corrected_pos = POS_CORRECTIONS.get(token.text, token.pos_)
            merged_pos_tags.append((token.text, corrected_pos))
            
            i += 1

    return {
        "tokens": merged_tokens,
        "lemmas": merged_lemmas,
        "pos_tags": merged_pos_tags,
        "entities": merged_entities
    }

# üìå Test du pr√©traitement avec une phrase
question = "Where Fort Knox located?"
preprocessed_output = preprocess_question(question)

# üìå Affichage des r√©sultats
for key, value in preprocessed_output.items():
    print(f"{key}: {value}")
